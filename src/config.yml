
MODEL:
    n_head: 8 
    n_layer: 12
    dropout: 0.1
    d_inner: 2048        #d_ff
    d_embed: 512
    d_model: 512
    dropatt: 0.0         #attention probability dropout rate
    query_dim: 16        #64
    seq_len: 512         #512
    n_token: 185
    mem_len: 512
    ext_len: 0
    tgt_len: 70
    eval_tgt_len: 50
    init: 'normal'       #parameter initializer to use.
    emb_init: 'normal'   #parameter initializer to use.
    init_range: 0.1
    emb_init_range: 0.01 #parameters initialized by U(-init_range, init_range)
    init_std: 0.02       #parameters initialized by N(0, init_std)
    proj_init_std: 0.01
    sample_softmax: -1
    clamp_len: -1        #use the same pos embeddings after clamp_len
    div_val: 1
    tied: False
    tie_projs: False
    position_concat: False
    adaptive_len: False
    pre_lnorm: True      #apply LayerNorm to the input instead of the output
    same_length: True    #use the same attn length for all tokens


TRAIN: 
    ROOT: 'datasets/775'
    gpuID: '1'
    output_dir: "./result"
    batch_size: 5  #5
    lr: 0.0002               #0.00025|5 for adam|sgd
    SCHEDULER: 'cosine'      #'cosine', 'inv_sqrt', 'dev_perf', 'constant'
    lr_min: 0.0
    eta_min: 0.0             #min learning rate for cosine scheduler
    decay_rate: 0.5          #decay factor when ReduceLROnPlateau is used
    max_step: 100000         #upper epoch limit
    max_eval_steps: -1       #max eval steps

    warmup_step: 0
    patience: 0
    no_cuda: False
    num_epochs: 600
    save_freq: 10
    seed: 2222
    optim: 'adam'           #'adam', 'sgd', 'adagrad'
    mom: 0.0                #momentum for sgd

    resume_training_model: None #'./result/20200717-065635/model_best.pth.tar'
    EARLY_STOP: 'specific_loss'  # 'specific_loss' or 'overfitting'
    


INFERENCE:
    num_sample: 95
    gpuID: '1'
    dictionary_path: '775/dictionary.pkl'
    experiment_dir: './result/20200810-160200/'   #20200722-062708
    checkpoint_type: epoch_idx        #best_train, best_val, epoch_idx
    model_epoch: 60  #460,
    no_cuda: False

